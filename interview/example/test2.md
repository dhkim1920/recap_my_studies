
### ## 🎯 데이터 엔지니어의 화이트보드 테스트 핵심 전략

**"어떻게 분석할 것인가?" (X)**
**"이 분석을 위해 어떤 데이터 파이프라인을 만들 것인가?" (O)**

이 관점으로 60분간의 준비와 발표를 설계해야 합니다.

---

### ## ⏰ 60분 준비 및 구상 전략 (엔지니어 Ver.)

주어진 문제(예: "물류센터의 피킹 효율을 높이기 위한 분석")를 받으면 아래 4단계로 생각을 정리하세요.

#### **1단계: 목표 및 결과물(Output) 정의 (10~15분)**
먼저 최종 결과물이 어떤 형태여야 하는지부터 역으로 설계합니다.

* **비즈니스 목표 확인**: "피킹 작업자의 동선을 최적화하여 시간당 처리량을 15% 늘린다."
* **최종 데이터 결과물(Output Table) 설계**: 분석가나 현업 담당자가 사용할 최종 테이블(Data Mart)의 모습을 먼저 그리세요. 이것이 바로 'Output 도식화'의 핵심입니다.
    * **예시 테이블: `picking_efficiency_mart`**
        | 컬럼명 | 데이터 타입 | 설명 |
        | :--- | :--- | :--- |
        | `center_id` | String | 물류센터 ID |
        | `worker_id` | String | 작업자 ID |
        | `work_date` | Date | 작업일자 |
        | `total_picking_time_sec` | Integer | 총 피킹 시간(초) |
        | `total_item_count` | Integer | 총 피킹 상품 수 |
        | `avg_item_per_hour` | Float | **[핵심 KPI]** 시간당 평균 피킹 수 |
        | `total_move_distance_m` | Integer | 총 이동 거리(미터) |

#### **2단계: 데이터 소스 및 수집 방식 구상 (10분)**
위 테이블을 만들려면 어떤 원천 데이터가 필요한지, 어떻게 가져올지 정의합니다.

* **필요 데이터 목록**:
    1.  **WMS DB**: 주문 정보, 상품 정보, 피킹 완료 시간 (`orders`, `products` 테이블)
    2.  **작업자 동선 로그**: 작업자 PDA/센서에서 발생하는 실시간 위치 데이터
* **수집 방식**:
    * WMS DB → **CDC (Change Data Capture)** 또는 **Airflow Batch**로 주기적 수집
    * 동선 로그 → **Kafka**로 실시간 스트리밍 수집

#### **3단계: 데이터 파이프라인 아키텍처 설계 (20~25분)**
이 부분이 **'접근 방식'**의 핵심이며, 화이트보드에 그릴 메인 그림입니다. 데이터가 어떻게 흘러가고 처리되는지 흐름도를 그리세요.



1.  **수집(Ingest)**: `Kafka`, `Debezium(CDC)`을 이용해 원천 데이터를 데이터 레이크로 수집.
2.  **처리(Process)**:
    * **Spark**를 사용해 두 데이터를 `JOIN`하고, 필요한 지표(`총 이동 거리`, `시간당 피킹 수` 등)를 계산.
    * 실시간성이 중요하다면 **Spark Structured Streaming**을, 아니라면 **Airflow**로 스케줄링된 **Spark Batch** 작업을 설계.
3.  **적재(Serve)**: 최종 처리된 결과를 **데이터 웨어하우스(DW)**의 `picking_efficiency_mart` 테이블에 적재.
4.  **활용(Usage)**: 이 마트 테이블을 **BI 툴(Tableau 등)**에 연결하여 시각화 대시보드를 만들거나, 데이터 분석가에게 제공.

#### **4단계: 고려사항 및 발표 준비 (10분)**
설계한 파이프라인의 기술적 트레이드오프나 개선점을 정리하며 발표를 준비합니다.

* **데이터 정합성**: WMS DB 데이터와 동선 로그 데이터의 시간 싱크가 맞지 않을 경우 어떻게 처리할 것인가?
* **확장성**: 데이터 양이 10배 늘어난다면 Spark 클러스터의 리소스나 Kafka 파티션은 어떻게 조정할 것인가?
* **실시간 vs 배치**: 왜 실시간(Streaming)이 아닌 배치(Batch)로 설계했는가? (또는 그 반대) 비용과 실효성의 트레이드오프를 설명.
* **모니터링**: Airflow DAG이 실패하거나 데이터가 늦게 들어올 때 어떻게 감지하고 알림을 보낼 것인가?

---

### ##  whiteboard 면접 발표 팁

* **결론부터 말하세요**: "저는 이 문제를 해결하기 위해 최종적으로 이러이러한 컬럼을 가진 데이터 마트를 설계했으며, 이 마트를 만들기 위한 데이터 파이프라인은 다음과 같습니다." 라고 시작하세요.
* **그림으로 설명하세요**: 화이트보드 중앙에 3단계에서 구상한 **아키텍처 다이어그램**을 크게 그리고, 데이터의 흐름을 따라가며 설명하세요.
* **'왜?'를 강조하세요**: "Kafka를 선택한 이유는 실시간 대용량 로그 수집에 적합하기 때문입니다.", "Spark를 사용한 이유는 비정형 로그 데이터와 정형 DB 데이터를 유연하게 조인할 수 있기 때문입니다." 와 같이 기술 선택의 이유를 명확히 밝히세요.

