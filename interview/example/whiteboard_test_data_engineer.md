
## 데이터 엔지니어의 화이트보드 테스트 핵심 전략

### 설계하는 관점에 대해
**"어떻게 분석할 것인가?"가 아닌 "이 분석을 위해 어떤 데이터 파이프라인을 만들 것인가?" 생각해야함**

## 문제 "물류센터의 피킹 효율을 높이기 위한 분석"

### 1단계: 목표 및 결과물(Output) 정의
먼저 최종 결과물이 어떤 형태여야 하는지부터 역으로 설계해 보자
- **비즈니스 목표 확인**: 피킹 작업자의 동선을 최적화하여 시간당 처리량을 15% 늘린다.
- **최종 데이터 결과물(Output Table) 설계**: 분석가나 현업 담당자가 사용할 최종 테이블(Data Mart)의 모습을 먼저 그리자, 이것이 바로 'Output 도식화'의 핵심이다.
  - **예시 테이블: `picking_efficiency_mart`**

| 컬럼명                  | 데이터 타입 | 설명                   |
| :---------------------- | :---------- | :--------------------- |
| `center_id`             | String      | 물류센터 ID            |
| `worker_id`             | String      | 작업자 ID              |
| `work_date`             | Date        | 작업일자               |
| `total_picking_time_sec`| Integer     | 총 피킹 시간(초)       |
| `total_item_count`      | Integer     | 총 피킹 상품 수        |
| `avg_item_per_hour`     | Float       | **[핵심 KPI]** 시간당 평균 피킹 수 |
| `total_move_distance_m` | Integer     | 총 이동 거리(미터)     |

### 2단계: 데이터 소스 및 수집 방식 구상
위 테이블을 만들려면 어떤 원천 데이터가 필요한지, 어떻게 가져올지 정의하자
- **필요 데이터 목록**
    1.  **WMS DB**: 주문 정보, 상품 정보, 피킹 완료 시간 (`orders`, `products` 테이블)
    2.  **작업자 동선 로그**: 작업자 PDA/센서에서 발생하는 실시간 위치 데이터
- **수집 방식**
    - WMS DB → CDC 또는 **Airflow Batch**로 주기적 수집
    - 동선 로그 → **Kafka**로 실시간 스트리밍 수집

### 3단계: 데이터 파이프라인 아키텍처 설계
데이터가 어떻게 흘러가고 처리되는지 흐름도를 그리자
1.  **수집(Ingest)**: `Kafka`, `Debezium(CDC)`을 이용해 원천 데이터를 데이터 레이크로 수집
2.  **처리(Process)**
    - **Spark**를 사용해 두 데이터를 `JOIN`하고, 필요한 지표(`총 이동 거리`, `시간당 피킹 수` 등)를 계산
    - 실시간성이 중요하다면 **Spark Structured Streaming**을, 아니라면 **Airflow**로 스케줄링된 Spark Batch 작업을 설계
3.  **적재(Serve)**: 최종 처리된 결과를 DW의 `picking_efficiency_mart` 테이블에 적재
4.  **활용(Usage)**: 이 마트 테이블을 BI 툴에 연결하여 시각화 대시보드를 만들거나, 데이터 분석가에게 제공

### 4단계: 고려사항 및 발표 준비
- **데이터 정합성**: WMS DB 데이터와 동선 로그 데이터의 시간 싱크가 맞지 않을 경우 어떻게 처리할 것인가?
- **확장성**: 데이터 양이 10배 늘어난다면 Spark 클러스터의 리소스나 Kafka 파티션은 어떻게 조정할 것인가?
- **실시간 vs 배치**: 왜 실시간(Streaming)이 아닌 배치(Batch)로 설계했는가? (또는 그 반대) 비용과 실효성의 트레이드오프를 설명
- **모니터링**: Airflow DAG이 실패하거나 데이터가 늦게 들어올 때 어떻게 감지하고 알림을 보낼 것인가?
