# 데이터 분석 및 머신러닝 주요 개념 정리

### **탐색적 데이터 분석 (EDA)**

* **일변량 분석 (Univariate Analysis):** **변수 하나**의 분포, 중심 경향, 퍼짐 정도 등을 파악하는 분석 (예: 히스토그램, 박스 플롯)
* **다변량 분석 (Multivariate Analysis):** **두 개 이상의 변수** 간의 관계나 상호작용을 파악하는 분석 (예: 산점도, 상관계수)

***

### **변수 간 관계 파악**

* **상관계수 (Correlation Coefficient):** 두 연속형 변수 간의 **선형 관계**의 강도와 방향을 나타내는 지표, Pandas에서는 `df.corr()` 메서드로 계산할 수 있다.
    * **1:** 완벽한 **양의 상관관계** (한 변수가 증가하면 다른 변수도 증가)
    * **-1:** 완벽한 **음의 상관관계** (한 변수가 증가하면 다른 변수는 감소)
    * **0:** 선형적인 상관관계 없음
* **교차표 (Crosstab):** 두 **범주형 변수** 간의 관계를 파악하기 위해, 각 범주 조합의 빈도를 표 형태로 나타낸 것
* **카이제곱 검정 (Chi-squared Test):** 교차표를 바탕으로 두 범주형 변수가 **서로 독립적인지 혹은 연관성이 있는지** 통계적으로 검정하는 방법

***

### **데이터 시각화 (Matplotlib)**

* **figure:** 전체 시각화가 그려지는 **캔버스 또는 영역**
* **plot:** figure 위에 그려지는 **실제 그래프**(차트, 그림 등)
* **show:** 완성된 시각화 객체(figure와 plot)를 **화면에 출력**

***

### **데이터 전처리 (Preprocessing)**

#### **결측치 (Missing Values) 처리**

결측값은 `isnull()` 메서드로 확인할 수 있으며, 처리 방법은 다음과 같다.

* **레코드 제거:** 특정 행(레코드)에 결측치가 있을 경우 해당 행을 삭제
* **컬럼 제거:** 특정 열(컬럼)의 결측치 비율이 너무 높을 때 (예: 50% 이상) 해당 열을 삭제
* **결측치 채워넣기 (Imputation):** 결측값을 다른 값(평균, 중앙값, 최빈값 등)으로 대체

#### **이상치 (Outliers) 처리**

* **IQR (Interquartile Range):** 데이터를 4등분했을 때 세 번째 사분위수($Q_3$)와 첫 번째 사분위수($Q_1$)의 차이($Q_3 - Q_1$)를 이용해 이상치를 탐지
* 보통 $Q_1 - 1.5 \times IQR$ 보다 작거나 $Q_3 + 1.5 \times IQR$ 보다 큰 값을 이상치로 간주한다.
* **Z-점수 (Z-score):** 데이터가 평균으로부터 몇 표준편차 떨어져 있는지를 나타내는 값
* 보통 Z-점수의 절댓값이 2 또는 3을 초과하면 이상치로 판단

#### **구간화 (Binning)**

연속형 변수를 일정한 구간으로 나누어 범주형 변수로 변환하는 작업

* **cut:** 사용자가 직접 **경곗값을 지정**하여 구간을 나눈다.
* **qcut:** 데이터의 **분위수(quantile)를 기준**으로 각 구간에 속하는 데이터의 개수를 비슷하게 나눈다.

#### **범주형 인코딩 (Categorical Encoding)**

문자열로 된 범주형 데이터를 머신러닝 모델이 이해할 수 있도록 숫자형으로 변환하는 것

* **원-핫 인코딩 (One-Hot Encoding):** 범주의 종류만큼 새로운 열을 만들고, 각 행에서 해당하는 범주에만 1을, 나머지에는 0을 표시하는 방식

#### **스케일링 (Scaling)**

서로 다른 변수들의 값의 범위를 일정한 수준으로 맞추는 작업, 변수 간의 스케일 차이가 모델 성능에 미치는 영향을 줄인다.

* **정규화 (Normalization):** 데이터를 0과 1 사이의 값으로 변환 (Min-Max Scaling)
* **표준화 (Standardization):** 데이터의 평균을 0, 표준편차를 1로 변환 (Z-score Standardization)

***

### **특성 선택 (Feature Selection)**

모델의 성능을 향상시키고 과적합을 방지하기 위해, 예측에 가장 중요한 변수(특성)들만 선택하는 과정\

* **RFE (Recursive Feature Elimination):** 모델을 반복적으로 학습시키면서 중요도가 가장 낮은 변수를 하나씩 제거하여 최적의 변수 조합을 찾는 방법
* **RFE-CV (RFE with Cross-Validation):** RFE에 교차 검증(Cross-Validation)을 추가하여, 제거할 변수의 개수를 자동으로 평가하고 최적의 개수를 찾아주는 더 안정적인 방법
* **단변량 특성 선택 (Univariate Feature Selection, UFS):** 각 변수를 개별적으로 평가하여 타겟 변수와의 통계적 관계가 높은 상위 변수들을 선택하는 방법 (예: 카이제곱 검정, 분산 분석(ANOVA) 등 활용)