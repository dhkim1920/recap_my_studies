
### 1. Rank

‘Rank’는 수학이나 통계, 머신러닝에서 행렬의 계수(선형독립한 행 또는 열의 개수)를 의미한다.
즉, 데이터가 가진 정보의 차원(독립적인 정보량)을 나타낸다.

**사용법**
- **선형대수:** 행렬 A의 rank는 `rank(A)`로 표현
- **AI/ML:** 데이터 차원 축소, 특이값 분해(SVD), PCA, 신경망 가중행렬 분석 등에서 사용된다.
  - 예: 낮은 rank를 가진 행렬은 중복된 정보가 많아 압축이 가능하다.

**원리**

행렬의 rank는 `linearly independent`한 벡터의 개수로 정의된다.
즉, 데이터의 독립적인 축이 몇 개인지를 의미한다.
→ rank가 낮으면 데이터가 특정 방향으로만 퍼져 있고, 차원이 실제보다 작다는 뜻이다.

---

### 2. Bayesian (베이즈 이론)

베이즈 정리는 **확률을 사후적으로 업데이트하는 원리**이다.
기존에 알고 있던 사전 확률(Prior)에 새로운 데이터(Evidence)가 주어졌을 때 사후 확률(Posterior)을 계산한다.

**공식**
P(A|B) = P(B|A) × P(A) / P(B)

**사용법**
- **AI/ML**
  - 나이브 베이즈 분류기(Naive Bayes Classifier)
  - 베이지안 네트워크(Bayesian Network)
  - 확률적 추론, 강화학습의 정책 업데이트 등
- **통계학:** 사후확률 기반의 추론, 불확실성 모델링에 사용

**원리**

새로운 증거가 관측될 때마다 기존의 믿음을 업데이트하여 **데이터에 따라 확률적으로 학습**하는 방식이다.
즉, 데이터가 많을수록 사후 확률이 실제값에 근접하게 된다.

---

### 3. PCA (Principal Component Analysis, 주성분 분석)

PCA는 **데이터의 분산을 최대화하는 축을 찾아 차원을 줄이는 방법**이다.
데이터의 패턴을 유지하면서도 노이즈나 중복된 정보를 제거한다.

**사용법**
- 고차원 데이터를 2~3차원으로 줄여 시각화
- 학습 전에 feature 간 상관관계 제거
- 이미지 압축, 음성 분석, anomaly detection 등

**원리**
1. 데이터의 공분산 행렬(Covariance matrix)을 계산
2. 고유값분해(Eigen decomposition) 수행
3. 가장 큰 고유값을 가진 축(Principal Component)을 선택
4. 데이터를 해당 축으로 사영(Projection) → 차원 축소

---

### 요약 관계

| 개념       | 주요 분야   | 핵심 원리           | AI에서의 역할          |
| -------- | ------- |-----------------| ----------------- |
| Rank     | 선형대수    | 독립된 정보의 수       | 데이터 구조 분석, 모델 압축  |
| Bayesian | 확률론     | 사전 → 사후 확률 업데이트 | 확률적 추론, 불확실성 처리   |
| PCA      | 통계/선형대수 | 분산 최대화 축 찾기     | 차원 축소, feature 압축 |

